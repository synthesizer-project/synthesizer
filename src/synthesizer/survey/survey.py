"""A module containing a pipeline helper class.

This module contains the Survey class which is used to run observable
generation pipelines on a set of galaxies. To use this functionality the user
needs to define the properties of the Survey and a function to load the
galaxies. After which the user can call the various methods to generate the
data they need, simplifying a complex pipeline full of boilerplate code to a
handle full of definitions and calls to the Survey object.

Example usage:
```python
    from synthesizer import Survey

    survey = Survey(
        gal_loader_func=load_galaxy,
        emission_model=emission_model,
        instruments=[instrument1, instrument2],
        n_galaxies=1000,
        nthreads=4,
        comm=None,
        verbose=1,
        )

    survey.load_galaxies()
    survey.get_spectra()
    survey.get_photometry_luminosities()
    survey.write("output.hdf5")
```
"""

import time
from functools import partial

import h5py
import numpy as np
from pathos.multiprocessing import ProcessingPool as Pool
from unyt import unyt_array

from synthesizer import check_openmp, exceptions
from synthesizer._version import __version__
from synthesizer.instruments.filters import FilterCollection
from synthesizer.survey.survey_utils import (
    recursive_gather,
    sort_data_recursive,
    write_datasets_recursive,
)
from synthesizer.utils.art import Art
from synthesizer.warnings import warn


class Survey:
    """
    A class for running observable generation pipelines on a set of galaxies.

    To use this class the user must instantiate it with a galaxy loading
    function, an emission model defining the different emissions that will be
    included in the pipeline, any instruments that will be used to make
    observations, and the number of galaxies that will be loaded.

    Optionally the user can also specify the number of threads to use if
    Synthesizer has been installed with OpenMP support, and an MPI communicator
    if they are running over MPI.

    Finally the verbosity level can be set to control the amount of output.

    Once the Survey object has been instantiated the user can call the various
    methods to generate the data they need.

    For spectra:
        - get_spectra (passing a cosmology object if redshifted spectra are
            required)
        - get_lnu_data_cubes (resolved spectral data cubes)
        - get_fnu_data_cubes (resolved spectral data cubes)

    For photometry:
        - get_photometry_luminosities
        - get_photometry_fluxes

    For emission lines:
        - get_lines (passing a list of line IDs to generate)

    For images (with optional PSF and noise based on the instrument):
        - get_images_luminosity
        - get_images_flux

    For the SFZH grid:
        - get_sfzh (passing a Grid object)


    The user can also add their own analysis functions to the pipeline which
    will be run on each galaxy once all data has been generated. These
    functions should take a galaxy object as the first argument and can take
    any number of additional arguments and keyword arguments. The results of
    these functions should be attached to the galaxy object, either as base
    level attributes or dictionaries containing the computed values. These
    attributes should be unique to the function to avoid overwriting existing
    attributes (they should be named what is passed to the result_attribute
    argument, see add_analysis_func for more details).

    Finally the user can write out the data generated by the pipeline using the
    write method. This will write out the data to an HDF5 file.

    Attributes:
        emission_model (EmissionModel):
            The emission model to use for the survey.
        instruments (list):
            A list of Instrument objects to use for the survey.
        n_galaxies (int):
            How many galaxies will we load in total (i.e. not per rank if using
            MPI)?
        nthreads (int):
            The number of threads to use for shared memory parallelism.
            Default is 1.
        comm (MPI.Comm):
            The MPI communicator to use for MPI parallelism. Default is None.
        verbose (int):
            How talkative are we?
            0: No output beyond hello and goodbye.
            1: Outputs with timings but only on rank 0 (when using MPI).
            2: Outputs with timings on all ranks (when using MPI).
        galaxies (list):
            A list of Galaxy objects that have been loaded.
        galaxy_indices (list):
            The indices of the galaxies that have been loaded.
        filters (FilterCollection):
            A combined collection of all the filters from the instruments.
    """

    def __init__(
        self,
        emission_model,
        instruments=(),
        nthreads=1,
        comm=None,
        verbose=1,
    ):
        """
        Initialise the Survey object.

        This will not perform any part of the calculation, it only sets it up.

        This will attach all the passed attributes of the survey and set up
        anything we'll need later like MPI variables (if applicable), flags
        to indicate what stages we've completed and containers for any
        ouputs and additional analysis functions.

        This will also check arguments are sensible, e.g.
            - The galaxy loader function is callable and takes at least one
              argument (the galaxy index).
            - Synthesizer has been installed with OpenMP support if multiple
              threads are requested.

        Args:
            emission_model (EmissionModel): The emission model to use for the
                survey.
            instruments (list): A list of Instrument objects to use for the
                survey.
            nthreads (int): The number of threads to use for shared memory
                parallelism. Default is 1.
            comm (MPI.Comm): The MPI communicator to use for MPI parallelism.
                Default is None.
            verbose (int): How talkative are we? 0: No output beyond hello and
                goodbye. 1: Outputs with timings but only on rank 0 (when using
                MPI). 2: Outputs with timings on all ranks (when using MPI).
        """
        # Attributes to track timing
        self._start_time = time.perf_counter()

        # Attach all the attributes we need to know what to do
        self.emission_model = emission_model
        self.instruments = instruments

        # Set verbosity
        self.verbose = verbose

        # How many galaxies are we going to be looking at?
        self.n_galaxies = 0
        self.n_galaxies_local = 0  # Only applicable when using MPI

        # Initialise an attribute we'll store our galaxy indices into (this
        # will either be 0-n_galaxies or a subset of these indices if we are
        # running with MPI). We'll construct this in load_galaxies.
        self.galaxy_indices = None

        # Define the container to hold the galaxies
        self.galaxies = []

        # How many threads are we using for shared memory parallelism?
        self.nthreads = nthreads

        # Check if we can use OpenMP
        if self.nthreads > 1 and not check_openmp():
            raise exceptions.MissingPartition(
                "Can't use multiple threads without OpenMP support. "
                " Install with: `WITH_OPENMP=1 pip install .`"
            )

        # It's quicker for us to collect together all filters and apply them
        # in one go, so we collect them together here. Note that getting
        # photometry is the only process that can be done collectively like
        # this without complex logic to check we don't have to do things
        # on an instrument-by-instrument basis (e.g. check resolution are
        # the same for imaging, wavelength arrays for spectroscopy etc.).
        self.filters = FilterCollection()
        for inst in instruments:
            if inst.can_do_photometry:
                self.filters += inst.filters

        # Define flags to indicate when we completed the various stages
        self._loaded_galaxies = False
        self._got_lnu_spectra = False
        self._got_fnu_spectra = False
        self._got_luminosities = False
        self._got_fluxes = False
        self._got_lum_lines = False
        self._got_flux_lines = False
        self._got_images_lum = False
        self._got_images_flux = False
        self._got_lnu_data_cubes = False
        self._got_fnu_data_cubes = False
        self._got_spectroscopy = False
        self._got_sfzh = False

        # Define containers for any additional analysis functions
        self._analysis_funcs = []
        self._analysis_args = []
        self._analysis_kwargs = []
        self._analysis_results_keys = []
        self._analysis_results = {}

        # It'll be helpful later if we know what line IDs have been requested
        # so we'll store these should get_lines be called.
        self._line_ids = []

        # Initialise containers for all the data we can generate
        self.sfzh = None
        self.lnu_spectra = {}
        self.fnu_spectra = {}
        self.luminosities = {}
        self.fluxes = {}
        self.lines_lum = {}
        self.line_cont_lum = {}
        self.flux_lines = {}
        self.images_lum = {}
        self.images_lum_psf = {}
        self.images_flux = {}
        self.images_flux_psf = {}
        self.lnu_data_cubes = {}
        self.fnu_data_cubes = {}
        self.spectroscopy = {}

        # Everything that follows is only needed for hybrid parallelism
        # (running with MPI in addition to shared memory parallelism)

        # If we are running with hybrid parallelism, we need to know about
        # the communicator for MPI
        self.comm = comm
        self.using_mpi = comm is not None

        # Get some MPI informaiton if we are using MPI
        if self.using_mpi:
            self.rank = comm.Get_rank()
            self.size = comm.Get_size()
        else:
            self.rank = 0
            self.size = 1

        if self.rank == 0:
            self._say_hello()
            self._report()

    def _validate_loader(self, func):
        """
        Validate the galaxy loader function.

        This function checks that the galaxy loader function is a callable
        and that it takes at least one argument (the galaxy index). If the
        function is not valid, an exception is raised.

        Args:
            func (callable): The galaxy loader function to validate.
        """
        # Ensure we have a callable function
        if not callable(func):
            raise exceptions.InconsistentArguments(
                "gal_loader_func is not a callable function"
                f" (found {type(func)})."
            )

        # Ensure we have at least 1 argument
        if len(func.__code__.co_varnames) < 1:
            raise exceptions.InconsistentArguments(
                "gal_loader_func must take at least one "
                f"argument (found {len(func.__code__.co_varnames)})."
            )

        # Ensure the first argument is called "gal_index"
        if func.__code__.co_varnames[0] != "gal_index":
            raise exceptions.InconsistentArguments(
                "The first argument of gal_loader_func must be the index "
                "of the galaxy to load and must be called 'gal_index'. "
                f"(found '{func.__code__.co_varnames[0]}')."
            )

        return func

    def _say_hello(self):
        """Print a nice welcome."""
        print()
        print("\n".join([" " * 25 + s for s in Art.galaxy.split("\n")]))
        print()

    def _report(self):
        """Print a report containing the Survey setup."""
        # Print the MPI setup if we are using MPI
        if self.using_mpi:
            self._print(f"Running with MPI on {self.size} ranks.")

        # Print the shared memory parallelism setup
        if self.nthreads > 1 and self.using_mpi:
            self._print(f"Running with {self.nthreads} threads per rank.")
        elif self.nthreads > 1:
            self._print(f"Running with {self.nthreads} threads.")

        # Print some information about the emission model
        self._print(f"Root emission model: {self.emission_model.label}")
        self._print(
            f"EmissionModel contains {len(self.emission_model._models)} "
            "individual models."
        )
        self._print("EmissionModels split by emitter:")
        label_width = max(
            len("   - galaxy"), len("   - stellar"), len("   - blackhole")
        )
        ngal_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m.emitter == "galaxy"
            ]
        )
        nstar_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m.emitter == "stellar"
            ]
        )
        nbh_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m.emitter == "blackhole"
            ]
        )
        self._print(f"{'   - galaxy:'.ljust(label_width + 2)} {ngal_models}")
        self._print(f"{'   - stellar:'.ljust(label_width + 2)} {nstar_models}")
        self._print(f"{'   - blackhole:'.ljust(label_width + 2)} {nbh_models}")

        self._print("EmissionModels split by operation type:")
        label_width = max(
            len("   - extraction"),
            len("   - combination"),
            len("   - attenuating"),
            len("   - generation"),
        )
        nextract_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m._is_extracting
            ]
        )
        ncombine_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m._is_combining
            ]
        )
        nattenuate_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m._is_dust_attenuating
            ]
        )
        ngen_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m._is_generating or m._is_dust_emitting
            ]
        )
        self._print(
            f"{'   - extraction:'.ljust(label_width + 2)} {nextract_models}"
        )
        self._print(
            f"{'   - combination:'.ljust(label_width + 2)} {ncombine_models}"
        )
        self._print(
            f"{'   - attenuating:'.ljust(label_width + 2)} {nattenuate_models}"
        )
        self._print(
            f"{'   - generation:'.ljust(label_width + 2)} {ngen_models}"
        )

        # Print the number of instruments we have
        self._print(f"Using {len(self.instruments)} instruments.")

        # Print the number of filters we have
        self._print(f"Instruments have {len(self.filters)} filters in total.")

        # Make a breakdown of the instruments
        self._print(
            "Included instruments:",
            ", ".join(list(self.instruments.instruments.keys())),
        )
        self._print("Instruments split by capability:")
        label_width = max(
            len("   - photometry"),
            len("   - spectroscopy"),
            len("   - imaging"),
            len("   - resolved spectroscopy"),
        )
        nphot_inst = len(
            [inst for inst in self.instruments if inst.can_do_photometry]
        )
        nspec_inst = len(
            [inst for inst in self.instruments if inst.can_do_spectroscopy]
        )
        nimg_inst = len(
            [inst for inst in self.instruments if inst.can_do_imaging]
        )
        nresspec_inst = len(
            [
                inst
                for inst in self.instruments
                if inst.can_do_resolved_spectroscopy
            ]
        )
        self._print(
            f"{'   - photometry:'.ljust(label_width + 2)} {nphot_inst}"
        )
        self._print(
            "   - spectroscopy:".ljust(label_width + 2),
            nspec_inst,
        )
        self._print(f"{'   - imaging:'.ljust(label_width + 2)} {nimg_inst}")
        self._print(
            f"{'   - resolved spectroscopy:'.ljust(label_width + 2)}"
            f" {nresspec_inst}"
        )

    def _say_goodbye(self):
        """Print a nice goodbye including timing."""
        elapsed = time.perf_counter() - self._start_time

        # Report in sensible units
        if elapsed < 1:
            elapsed *= 1000
            units = "ms"
        elif elapsed < 60:
            units = "s"
        elif elapsed < 3600:
            elapsed /= 60
            units = "mins"
        else:
            elapsed /= 3600
            units = "hours"

        # Report how blazingly fast we are
        self._print(f"Total synthesis took {elapsed:.3f} {units}.")
        self._print("Goodbye!")

    def _print(self, *args, **kwargs):
        """
        Print a message to the screen with extra information.

        The prints behave differently depending on whether we are using MPI or
        not. We can also set the verbosity level at the Survey level which will
        control the verbosity of the print statements.

        Verbosity:
            0: No output beyond hello and goodbye.
            1: Outputs with timings but only on rank 0 (when using MPI).
            2: Outputs with timings on all ranks (when using MPI).

        Args:
            message (str): The message to print.
        """
        # At verbosity 0 we are silent
        if self.verbose == 0:
            return

        # Get the current time code in seconds with 0 padding and 2
        # decimal places
        now = time.perf_counter() - self._start_time
        int_now = str(int(now)).zfill(
            len(str(int(now))) + 1 if now > 9999 else 5
        )
        decimal = str(now).split(".")[-1][:2]
        now_str = f"{int_now}.{decimal}"

        # Create the prefix for the print, theres extra info to output if
        # we are using MPI
        if self.using_mpi:
            # Only print on rank 0 if we are using MPI and have verbosity 1
            if self.verbose == 1 and self.rank != 0:
                return

            prefix = (
                f"[{str(self.rank).zfill(len(str(self.size)) + 1)}]"
                f"[{now_str}]:"
            )

        else:
            prefix = f"[{now_str}]:"

        print(prefix, *args, **kwargs)

    def _took(self, start, message):
        """
        Print a message with the time taken since the start time.

        Args:
            start (float): The start time of the process.
            message (str): The message to print.
        """
        elapsed = time.perf_counter() - start

        # Report in sensible units
        if elapsed < 1:
            elapsed *= 1000
            units = "ms"
        elif elapsed < 60:
            units = "s"
        else:
            elapsed /= 60
            units = "mins"

        # Report how blazingly fast we are
        self._print(f"{message} took {elapsed:.3f} {units}.")

    def add_analysis_func(self, func, result_key, *args, **kwargs):
        """
        Add an analysis function to the Survey.

        The provided function will be called on each galaxy in the Survey once
        all data has been generated. The function should take a galaxy object
        as the first argument and can take any number of additional arguments
        and keyword arguments.

        The results of the analysis function should be returned. This can
        be a scalar, array, or a dictionary of arbitrary structure. We'll
        store it in a dictionary on the Survey object with the key being the
        result_key argument.

        For example:

            ```python
            def my_analysis_func(galaxy, *args, **kwargs):
                return galaxy.some_attribute * 2

            survey.add_analysis_func(my_analysis_func, "MyAnalysisResult")
            ```

        Or for a specific component of the galaxy:

            ```python
            def my_analysis_func(galaxy, *args, **kwargs):
                return galaxy.stars.mass.sum()

            survey.add_analysis_func(my_analysis_func, "Stars/Mass")
            ```

        Args:
            func (callable):
                The analysis function to add to the Survey. This function
                should take a galaxy object as the first argument and can take
                any number of additional arguments and keyword arguments.
            result_key (str):
                The key to use when storing the results of the analysis
                function in the output. This can include slashes to denote
                nesting, e.g. "Gas/Nested/Result".
        """
        # Ensure we have a callable function
        if not callable(func):
            raise exceptions.InconsistentArguments(
                "Analysis function is not a callable function"
                f" (found {type(func)})."
            )

        # Warn the user if theres a name clash, we'll take the new one
        if func.__name__ in self._analysis_funcs:
            warn(
                f"{func.__name__} already exists in the analysis functions. "
                "Overwriting with the passed function."
            )

        # Add the function to the dictionary
        self._analysis_funcs.append(func)
        self._analysis_args.append(args)
        self._analysis_kwargs.append(kwargs)
        self._analysis_results_keys.append(result_key)

        # Warn the user if theres a name clash, we'll take the new one
        if result_key in self._analysis_results:
            raise exceptions.InconsistentArguments(
                f"{result_key} already exists in the analysis results. "
                "Choose a different result_key"
            )
        else:
            self._analysis_results[result_key] = None

        self._print(f"Added analysis function: {result_key}")

    def add_galaxies(self, galaxies, galaxy_indices=None):
        """
        Add galaxies to the Survey.

        This function will add the provided galaxies to the Survey. This is
        useful if you have already loaded the galaxies and want to add them to
        the Survey object.

        Args:
            galaxies (list):
                A list of Galaxy objects to add to the Survey.
            galaxy_indices (list):
                The indices of the galaxies to add. If None, the indices will
                be generated based on the length of the galaxies list. If
                using MPI, then indices will be generated in rank order.
        """
        start = time.perf_counter()

        # Attach the galaxies
        self.galaxies = galaxies
        self.n_galaxies_local = len(galaxies)

        # If we're in MPI land we need sum the local counts across all ranks
        # to get the total number of galaxies
        if self.using_mpi:
            self.n_galaxies = self.comm.allreduce(self.n_galaxies_local)
        else:
            self.n_galaxies = self.n_galaxies_local

        # If we don't have galaxy indices, generate them
        if galaxy_indices is None:
            galaxy_indices = list(range(self.n_galaxies_local))

        # Make sure we have an array of indices
        galaxy_indices = np.array(galaxy_indices)

        # Make sure the indices number of galaxies match the number of indices
        if len(galaxy_indices) != self.n_galaxies_local:
            raise exceptions.InconsistentArguments(
                "The number of galaxy indices does not match the number of "
                f"galaxies provided (found len(galaxy_indices) "
                f"= {len(galaxy_indices)} and len(galaxies) "
                f"= {self.n_galaxies_local})."
            )

        # If we're in MPI land we need to shift the indices so they are
        # unique and contiguous across all ranks
        if self.using_mpi:
            # Get the counts from all ranks
            counts = self.comm.allgather(self.n_galaxies_local)

            # Get the offset for this rank
            ind_offset = sum(counts[: self.rank])

            # Shift the indices
            galaxy_indices += ind_offset

        # Attach the galaxy indices
        self.galaxy_indices = galaxy_indices

        # If we have MPI lets report the balance
        if self.using_mpi:
            self._report_balance()

        # Done!
        self._loaded_galaxies = True
        self._took(start, f"Adding {self.n_galaxies} galaxies")

    def get_los_optical_depths(
        self, kernel, kernel_threshold=1.0, kappa=0.0795
    ):
        """
        Compute the Line of Sight optical depths for all particles.

        This will compute the optical depths based on the line of sight dust
        column density for all non-gas components. We project a ray along the
        z axis (LOS) and any gas kernels it intersects are evaluated at the
        intersection and their contributions to the optical depth is included.

        Args:
            kernel (array-like):
                The gas SPH kernel.
            kernel_threshold (float):
                The threshold of the kernel. Default is 1.0.
            kappa (float):
                The dust opacity coefficient in units of Msun / pc**2. Default
                is 0.0795.
        """
        start = time.perf_counter()

        # Loop over galaxies and compute the optical depths for the present
        # components. This can use internal shared memory parallelism so we
        # just loop over the galaxies.
        for g in self.galaxies:
            g.get_stellar_los_tau_v(
                kappa=kappa,
                kernel=kernel,
                threshold=kernel_threshold,
                nthreads=self.nthreads,
            )
            g.get_black_hole_los_tau_v(
                kappa=kappa,
                kernel=kernel,
                threshold=kernel_threshold,
                nthreads=self.nthreads,
            )

        # Done!
        self._took(start, "Getting LOS optical depths")

    def get_sfzh(self, grid):
        """
        Compute the SFZH grid for each galaxy.

        This is also the integrated weights of each star particle onto the SPS
        grid.

        Args:
            grid (Grid):
                The SPS grid to use for the SFZH calculation.
        """
        start = time.perf_counter()

        # Loop over galaxies and get thier SFZH, skip any without stars. This
        # Can use internal shared memory parallelism so we just loop over the
        # galaxies.
        for g in self.galaxies:
            # Parametric galaxies have this ready to go so we can skip them
            if getattr(g, "sfzh", None) is not None:
                continue
            elif g.stars is not None and g.stars.nstars > 0:
                g.get_sfzh(grid, nthreads=self.nthreads)

        # Unpack the SFZH attributes into a single array on the Survey object
        self.sfzh = unyt_array(
            [g.sfzh.value for g in self.galaxies],
            self.galaxies[0].sfzh.units,
        )

        # Done!
        self._got_sfzh = True
        self._took(start, "Getting SFZH")

    def get_spectra(self, cosmo=None):
        """Generate the spectra for the galaxies based on the EmissionModel."""
        start = time.perf_counter()

        # Ensure we are ready
        if not self._loaded_galaxies:
            raise exceptions.SurveyNotReady(
                "Cannot generate spectra before galaxies are loaded! "
                "Call load_galaxies first."
            )

        # Loop over the galaxies and get the spectra
        for g in self.galaxies:
            g.get_spectra(self.emission_model, nthreads=self.nthreads)

        # If we have a cosmology, get the observed spectra. We can do this
        # with a threadpool if we have multiple threads, but there's no
        # internal shared memory parallelism in this process.
        if cosmo is not None and self.nthreads > 1:

            def _get_observed_spectra(g):
                g.get_observed_spectra(cosmo=cosmo)
                return g

            with Pool(self.nthreads) as pool:
                self.galaxies = pool.map(
                    _get_observed_spectra,
                    self.galaxies,
                )
        elif cosmo is not None:
            for g in self.galaxies:
                g.get_observed_spectra(cosmo=cosmo)

        # Ubpack the spectra into a dictionary on the Survey object
        self.lnu_spectra = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        self.fnu_spectra = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        for g in self.galaxies:
            for spec_type, spec in g.spectra.items():
                self.lnu_spectra["Galaxy"].setdefault(spec_type, []).append(
                    spec.lnu
                )
                if cosmo is not None:
                    self.fnu_spectra["Galaxy"].setdefault(
                        spec_type, []
                    ).append(spec.fnu)
            if g.stars is not None:
                for spec_type, spec in g.stars.spectra.items():
                    self.lnu_spectra["Stars"].setdefault(spec_type, []).append(
                        spec.lnu
                    )
                    if cosmo is not None:
                        self.fnu_spectra["Stars"].setdefault(
                            spec_type, []
                        ).append(spec.fnu)
            if g.black_holes is not None:
                for spec_type, spec in g.black_holes.spectra.items():
                    self.lnu_spectra["BlackHole"].setdefault(
                        spec_type, []
                    ).append(spec.lnu)
                    if cosmo is not None:
                        self.fnu_spectra["BlackHole"].setdefault(
                            spec_type, []
                        ).append(spec.fnu)

        # Convert the lists of spectra to unyt arrays
        for spec_type, spec in self.lnu_spectra["Galaxy"].items():
            self.lnu_spectra["Galaxy"][spec_type] = unyt_array(spec)
        for spec_type, spec in self.lnu_spectra["Stars"].items():
            self.lnu_spectra["Stars"][spec_type] = unyt_array(spec)
        for spec_type, spec in self.lnu_spectra["BlackHole"].items():
            self.lnu_spectra["BlackHole"][spec_type] = unyt_array(spec)
        for spec_type, spec in self.fnu_spectra["Galaxy"].items():
            self.fnu_spectra[spec_type] = unyt_array(spec)
        for spec_type, spec in self.fnu_spectra["Stars"].items():
            self.fnu_spectra["Stars"][spec_type] = unyt_array(spec)
        for spec_type, spec in self.fnu_spectra["BlackHole"].items():
            self.fnu_spectra["BlackHole"][spec_type] = unyt_array(spec)

        # Done!
        self._got_lnu_spectra = True
        self._got_fnu_spectra = True if cosmo is not None else False
        self._took(start, "Generating spectra")

    def get_photometry_luminosities(self):
        """Compute the photometric luminosities from the generated spectra."""
        start = time.perf_counter()

        # Ensure we are ready
        if not self._got_lnu_spectra:
            raise exceptions.SurveyNotReady(
                "Cannot generate photometry before lnu spectra are generated! "
                "Call get_spectra first."
            )
        elif len(self.filters) == 0:
            raise exceptions.SurveyNotReady(
                "Cannot generate photometry without instruments! "
                "Add instruments with filters and try again."
            )

        # Loop over the galaxies and get the photometry, there is internal
        # shared memory parallelism in this process so we can just loop over
        # the galaxies at this level
        for g in self.galaxies:
            g.get_photo_lnu(filters=self.filters, nthreads=self.nthreads)

        # Unpack the luminosities into a dictionary on the Survey object
        self.luminosities = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        for g in self.galaxies:
            for spec_type, phot in g.photo_lnu.items():
                for filt, lnu in phot.items():
                    self.luminosities["Galaxy"].setdefault(
                        spec_type, {}
                    ).setdefault(filt, []).append(lnu)
            if g.stars is not None:
                for spec_type, phot in g.stars.photo_lnu.items():
                    for filt, lnu in phot.items():
                        self.luminosities["Stars"].setdefault(
                            spec_type, {}
                        ).setdefault(filt, []).append(lnu)
            if g.black_holes is not None:
                for spec_type, phot in g.black_holes.photo_lnu.items():
                    for filt, lnu in phot.items():
                        self.luminosities["BlackHole"].setdefault(
                            spec_type, {}
                        ).setdefault(filt, []).append(lnu)

        # Convert the lists of luminosities to unyt arrays
        for spec_type, phot in self.luminosities["Galaxy"].items():
            for filt, lnu in phot.items():
                self.luminosities["Galaxy"][spec_type][filt] = unyt_array(lnu)
        for spec_type, phot in self.luminosities["Stars"].items():
            for filt, lnu in phot.items():
                self.luminosities["Stars"][spec_type][filt] = unyt_array(lnu)
        for spec_type, phot in self.luminosities["BlackHole"].items():
            for filt, lnu in phot.items():
                self.luminosities["BlackHole"][spec_type][filt] = unyt_array(
                    lnu
                )

        # Done!
        self._got_luminosities = True
        self._took(start, "Getting photometric luminosities")

    def get_photometry_fluxes(self):
        """Compute the photometric fluxes from the generated spectra."""
        start = time.perf_counter()

        # Ensure we are ready
        if not self._got_fnu_spectra:
            raise exceptions.SurveyNotReady(
                "Cannot generate photometry before fnu spectra are generated! "
                "Call get_spectra with a cosmology object first."
            )
        elif len(self.filters) == 0:
            raise exceptions.SurveyNotReady(
                "Cannot generate photometry without instruments! "
                "Add instruments with filters and try again."
            )

        # Loop over the galaxies and get the photometry, there is internal
        # shared memory parallelism in this process so we can just loop over
        # the galaxies at this level
        for g in self.galaxies:
            g.get_photo_fnu(filters=self.filters, nthreads=self.nthreads)

        # Unpack the fluxes into a dictionary on the Survey object
        self.fluxes = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        for g in self.galaxies:
            for spec_type, phot in g.photo_fnu.items():
                for filt, fnu in phot.items():
                    self.fluxes["Galaxy"].setdefault(spec_type, {}).setdefault(
                        filt, []
                    ).append(fnu)
            if g.stars is not None:
                for spec_type, phot in g.stars.photo_fnu.items():
                    for filt, fnu in phot.items():
                        self.fluxes["Stars"].setdefault(
                            spec_type, {}
                        ).setdefault(filt, []).append(fnu)
            if g.black_holes is not None:
                for spec_type, phot in g.black_holes.photo_fnu.items():
                    for filt, fnu in phot.items():
                        self.fluxes["BlackHole"].setdefault(
                            spec_type, {}
                        ).setdefault(filt, []).append(fnu)

        # Convert the lists of fluxes to unyt arrays
        for spec_type, phot in self.fluxes["Galaxy"].items():
            for filt, fnu in phot.items():
                self.fluxes["Galaxy"][spec_type][filt] = unyt_array(fnu)
        for spec_type, phot in self.fluxes["Stars"].items():
            for filt, fnu in phot.items():
                self.fluxes["Stars"][spec_type][filt] = unyt_array(fnu)
        for spec_type, phot in self.fluxes["BlackHole"].items():
            for filt, fnu in phot.items():
                self.fluxes["BlackHole"][spec_type][filt] = unyt_array(fnu)

        # Done!
        self._got_fluxes = True
        self._took(start, "Getting photometric fluxes")

    def get_lines(self, line_ids):
        """
        Generate the emission lines for the galaxies.

        This function will generate the emission lines for all spectra types
        that were saved when spectra were generated.

        Args:
            line_ids (list):
                The emission line IDs to generate.
        """
        start = time.perf_counter()

        self._print(f"Generating {len(line_ids)} emission lines...")

        # Ensure we are ready
        if not self._loaded_galaxies:
            raise exceptions.SurveyNotReady(
                "Cannot generate emission lines before galaxies are loaded! "
                "Call load_galaxies first."
            )

        # Loop over the galaxies and get the spectra
        for g in self.galaxies:
            g.get_lines(line_ids, self.emission_model, nthreads=self.nthreads)

        # Store the line IDs for later
        self._line_ids = line_ids

        # Unpack the luminosity lines into a dictionary on the Survey object
        self.lines_lum = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        self.line_cont_lum = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        for g in self.galaxies:
            for spec_type, lines in g.lines.items():
                for line in lines:
                    self.lines_lum["Galaxy"].setdefault(
                        spec_type, {}
                    ).setdefault(line.id, []).append(line.luminosity)
                    self.line_cont_lum["Galaxy"].setdefault(
                        spec_type, {}
                    ).setdefault(line.id, []).append(line.continuum)
            if g.stars is not None:
                for spec_type, lines in g.stars.lines.items():
                    for line in lines:
                        self.lines_lum["Stars"].setdefault(
                            spec_type, {}
                        ).setdefault(line.id, []).append(line.luminosity)
                        self.line_cont_lum["Stars"].setdefault(
                            spec_type, {}
                        ).setdefault(line.id, []).append(line.continuum)
            if g.black_holes is not None:
                for spec_type, lines in g.black_holes.lines.items():
                    for line in lines:
                        self.lines_lum["BlackHole"].setdefault(
                            spec_type, {}
                        ).setdefault(line.id, []).append(line.luminosity)
                        self.line_cont_lum["BlackHole"].setdefault(
                            spec_type, {}
                        ).setdefault(line.id, []).append(line.continuum)

        # Convert the lists of luminosities to unyt arrays
        for spec_type, lines in self.lines_lum["Galaxy"].items():
            for line_id, lum in lines.items():
                self.lines_lum["Galaxy"][spec_type][line_id] = unyt_array(lum)
        for spec_type, lines in self.lines_lum["Stars"].items():
            for line_id, lum in lines.items():
                self.lines_lum["Stars"][spec_type][line_id] = unyt_array(lum)
        for spec_type, lines in self.lines_lum["BlackHole"].items():
            for line_id, lum in lines.items():
                self.lines_lum["BlackHole"][spec_type][line_id] = unyt_array(
                    lum
                )

        # Done!
        self._got_lum_lines = True
        self._took(start, "Getting emission lines")

    def get_images_luminosity(
        self,
        fov,
        img_type="smoothed",
        kernel=None,
        kernel_threshold=1.0,
    ):
        """
        Compute the luminosity images for the galaxies.

        This function will compute the luminosity images for all spectra types
        that were saved when spectra were generated, in all filters included in
        the Survey instruments.

        A PSF and/or noise will be applied if they are available on the
        instrument.

        Args:
            fov (unyt_quantity):
                The field of view of the image with units.
            img_type (str):
                The type of image to generate. Options are 'smoothed' or
                'hist'. Default is 'smoothed'.
            kernel (array-like):
                The kernel to use for smoothing the image. Default is None.
                Required for 'smoothed' images from a particle distribution.
            kernel_threshold (float):
                The threshold of the kernel. Default is 1.0.
        """
        start = time.perf_counter()

        # Ensure we are ready
        if not self._got_luminosities:
            raise exceptions.SurveyNotReady(
                "Cannot generate images before luminosities are generated! "
                "Call get_photometry_luminosities first."
            )

        def _apply_psfs(g, psfs):
            """
            Apply a PSF to all the images in the galaxy and its components.

            Args:
                g (Galaxy):
                    The galaxy to apply the PSF to.
                psfs (dict):
                    The PSFs to apply to the images.
            """
            psfd_imgs = getattr(g, "images_psf_lnu", {})
            for key, imgs in g.images_lnu.items():
                psfd_imgs.setdefault(key, {})
                for f, img in imgs.items():
                    if f in psfs:
                        psfd_imgs[key][f] = img.apply_psf(psfs[f])
            g.images_psf_lnu = psfd_imgs

            if g.stars is not None:
                psfd_imgs = getattr(g.stars, "images_psf_lnu", {})
                for key, imgs in g.stars.images_lnu.items():
                    psfd_imgs.setdefault(key, {})
                    for f, img in imgs.items():
                        if f in psfs:
                            psfd_imgs[key][f] = img.apply_psf(psfs[f])
                g.stars.images_psf_lnu = psfd_imgs

            if g.black_holes is not None:
                psfd_imgs = getattr(g.black_holes, "images_psf_lnu", {})
                for key, imgs in g.black_holes.images_lnu.items():
                    psfd_imgs.setdefault(key, {})
                    for f, img in imgs.items():
                        if f in psfs:
                            psfd_imgs[key][f] = img.apply_psf(psfs[f])
                g.black_holes.images_psf_lnu = psfd_imgs

        def _apply_noise(g):
            """"""
            for inst in self.instruments:
                if inst.can_do_noisy_imaging:
                    for img in g.images_lnu:
                        img.apply_noise(inst.noise_maps)

        # Loop over instruments and perform any imaging they define
        for inst in self.instruments:
            # Skip if the instrument can't do imaging
            if not inst.can_do_imaging:
                continue

            # Loop over galaxies getting the initial images. We do this on
            # an individual galaxy basis since we can use internal shared
            # memory parallelism to do this
            for g in self.galaxies:
                g.get_images_luminosity(
                    resolution=inst.resolution,
                    fov=fov,
                    emission_model=self.emission_model,
                    img_type=img_type,
                    kernel=kernel,
                    kernel_threshold=kernel_threshold,
                    nthreads=self.nthreads,
                )

            # If the instrument has a PSF we can apply that here. If we can
            # we'll use a pool of threads to do this in parallel since there
            # is no internal shared memory parallelism in this process.
            if inst.can_do_psf_imaging:
                # Do we have multiple threads?
                if self.nthreads > 1:
                    with Pool(self.nthreads) as pool:
                        pool.map(
                            partial(_apply_psfs, psfs=inst.psfs),
                            self.galaxies,
                        )
                else:
                    for g in self.galaxies:
                        _apply_psfs(g, inst.psfs)

            # If the instrument has noise we can apply that here. Again, if
            # we can we'll use a pool of threads to do this in parallel since
            # there is no internal shared memory parallelism in this process.
            if inst.can_do_noisy_imaging:
                # Do we have multiple threads?
                if self.nthreads > 1:
                    with Pool(self.nthreads) as pool:
                        pool.map(_apply_noise, self.galaxies)
                else:
                    for g in self.galaxies:
                        _apply_noise(instrument=inst)

        # Unpack the luminosity images into a dictionary on the Survey object
        self.images_lum = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        self.images_lum_psf = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        for g in self.galaxies:
            for spec_type, imgs in g.images_lnu.items():
                for f, img in imgs.items():
                    self.images_lum["Galaxy"].setdefault(
                        spec_type, {}
                    ).setdefault(f, []).append(img.arr * img.units)
            if hasattr(g, "images_psf_lnu"):
                for spec_type, imgs in g.images_psf_lnu.items():
                    for f, img in imgs.items():
                        self.images_lum_psf["Galaxy"].setdefault(
                            spec_type, {}
                        ).setdefault(f, []).append(img.arr * img.units)
            if g.stars is not None:
                for spec_type, imgs in g.stars.images_lnu.items():
                    for f, img in imgs.items():
                        self.images_lum["Stars"].setdefault(
                            spec_type, {}
                        ).setdefault(f, []).append(img.arr * img.units)
                if hasattr(g.stars, "images_psf_lnu"):
                    for spec_type, imgs in g.stars.images_psf_lnu.items():
                        for f, img in imgs.items():
                            self.images_lum_psf["Stars"].setdefault(
                                spec_type, {}
                            ).setdefault(f, []).append(img.arr * img.units)
            if g.black_holes is not None:
                for spec_type, imgs in g.black_holes.images_lnu.items():
                    for f, img in imgs.items():
                        self.images_lum["BlackHole"].setdefault(
                            spec_type, {}
                        ).setdefault(f, []).append(img.arr * img.units)
                if hasattr(g.black_holes, "images_psf_lnu"):
                    for (
                        spec_type,
                        imgs,
                    ) in g.black_holes.images_psf_lnu.items():
                        for f, img in imgs.items():
                            self.images_lum_psf["BlackHole"].setdefault(
                                spec_type, {}
                            ).setdefault(f, []).append(img.arr * img.units)

        # Convert the lists of images to unyt arrays
        for spec_type, imgs in self.images_lum["Galaxy"].items():
            for f, img in imgs.items():
                self.images_lum["Galaxy"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_lum["Stars"].items():
            for f, img in imgs.items():
                self.images_lum["Stars"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_lum["BlackHole"].items():
            for f, img in imgs.items():
                self.images_lum["BlackHole"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_lum_psf.items():
            for f, img in imgs.items():
                self.images_lum_psf[spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_lum_psf["Stars"].items():
            for f, img in imgs.items():
                self.images_lum_psf["Stars"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_lum_psf["BlackHole"].items():
            for f, img in imgs.items():
                self.images_lum_psf["BlackHole"][spec_type][f] = unyt_array(
                    img
                )

        # Done!
        self._got_images_lum = True
        self._took(start, "Getting luminosity images")

    def get_images_flux(
        self,
        fov,
        img_type="smoothed",
        kernel=None,
        kernel_threshold=1.0,
    ):
        """
        Compute the flux images for the galaxies.

        This function will compute the flux images for all spectra types
        that were saved when spectra were generated, in all filters included
        in the Survey instruments.

        A PSF and/or noise will be applied if they are available on the
        instrument.

        Args:
            fov (unyt_quantity):
                The field of view of the image with units.
            img_type (str):
                The type of image to generate. Options are 'smoothed' or
                'hist'. Default is 'smoothed'.
            kernel (array-like):
                The kernel to use for smoothing the image. Default is None.
                Required for 'smoothed' images from a particle distribution.
            kernel_threshold (float):
                The threshold of the kernel. Default is 1.0.
        """
        start = time.perf_counter()

        # Ensure we are ready
        if not self._got_fluxes:
            raise exceptions.SurveyNotReady(
                "Cannot generate images before fluxes are generated! "
                "Call get_photometry_fluxes first."
            )

        def _apply_psfs(g, psfs):
            """
            Apply a PSF to all the images in the galaxy and its components.

            Args:
                g (Galaxy):
                    The galaxy to apply the PSF to.
                psfs (dict):
                    The PSFs to apply to the images.
            """
            psfd_imgs = getattr(g, "images_psf_fnu", {})
            for key, imgs in g.images_fnu.items():
                psfd_imgs.setdefault(key, {})
                for f, img in imgs.items():
                    if f in psfs:
                        psfd_imgs[key][f] = img.apply_psf(psfs[f])
            g.images_psf_fnu = psfd_imgs

            if g.stars is not None:
                psfd_imgs = getattr(g.stars, "images_psf_fnu", {})
                for key, imgs in g.stars.images_fnu.items():
                    psfd_imgs.setdefault(key, {})
                    for f, img in imgs.items():
                        if f in psfs:
                            psfd_imgs[key][f] = img.apply_psf(psfs[f])
                g.stars.images_psf_fnu = psfd_imgs

            if g.black_holes is not None:
                psfd_imgs = getattr(g.black_holes, "images_psf_fnu", {})
                for key, imgs in g.black_holes.images_fnu.items():
                    psfd_imgs.setdefault(key, {})
                    for f, img in imgs.items():
                        if f in psfs:
                            psfd_imgs[key][f] = img.apply_psf(psfs[f])
                g.black_holes.images_psf_fnu = psfd_imgs

        def _apply_noise(g):
            """"""
            for inst in self.instruments:
                if inst.can_do_noisy_imaging:
                    for img in g.images_fnu:
                        img.apply_noise(inst.noise_maps)

        # Loop over instruments and perform any imaging they define
        for inst in self.instruments:
            # Skip if the instrument can't do imaging
            if not inst.can_do_imaging:
                continue

            # Loop over galaxies getting the initial images. We do this on
            # an individual galaxy basis since we can use internal shared
            # memory parallelism to do this
            for g in self.galaxies:
                g.get_images_flux(
                    resolution=inst.resolution,
                    fov=fov,
                    emission_model=self.emission_model,
                    img_type=img_type,
                    kernel=kernel,
                    kernel_threshold=kernel_threshold,
                    nthreads=self.nthreads,
                )

            # If the instrument has a PSF we can apply that here. If we can
            # we'll use a pool of threads to do this in parallel since there
            # is no internal shared memory parallelism in this process.
            if inst.can_do_psf_imaging:
                # Do we have multiple threads?
                if self.nthreads > 1:
                    with Pool(self.nthreads) as pool:
                        pool.map(
                            partial(_apply_psfs, psfs=inst.psfs),
                            self.galaxies,
                        )
                else:
                    for g in self.galaxies:
                        _apply_psfs(g)

            # If the instrument has noise we can apply that here. Again, if
            # we can we'll use a pool of threads to do this in parallel since
            # there is no internal shared memory parallelism in this process.
            if inst.can_do_noisy_imaging:
                # Do we have multiple threads?
                if self.nthreads > 1:
                    with Pool(self.nthreads) as pool:
                        pool.map(_apply_noise, self.galaxies)
                else:
                    for g in self.galaxies:
                        _apply_noise(g)

        # Unpack the luminosity images into a dictionary on the Survey object
        self.images_flux = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        self.images_flux_psf = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        for g in self.galaxies:
            for spec_type, imgs in g.images_fnu.items():
                for f, img in imgs.items():
                    self.images_flux["Galaxy"].setdefault(
                        spec_type, {}
                    ).setdefault(f, []).append(img.arr * img.units)
            if hasattr(g, "images_psf_fnu"):
                for spec_type, imgs in g.images_psf_fnu.items():
                    for f, img in imgs.items():
                        self.images_flux_psf["Galaxy"].setdefault(
                            spec_type, {}
                        ).setdefault(f, []).append(img.arr * img.units)
            if g.stars is not None:
                for spec_type, imgs in g.stars.images_fnu.items():
                    for f, img in imgs.items():
                        self.images_flux["Stars"].setdefault(
                            spec_type, {}
                        ).setdefault(f, []).append(img.arr * img.units)
                if hasattr(g.stars, "images_psf_fnu"):
                    for spec_type, imgs in g.stars.images_psf_fnu.items():
                        for f, img in imgs.items():
                            self.images_flux_psf["Stars"].setdefault(
                                spec_type, {}
                            ).setdefault(f, []).append(img.arr * img.units)
            if g.black_holes is not None:
                for spec_type, imgs in g.black_holes.images_fnu.items():
                    for f, img in imgs.items():
                        self.images_flux["BlackHole"].setdefault(
                            spec_type, {}
                        ).setdefault(f, []).append(img.arr * img.units)
                if hasattr(g.black_holes, "images_psf_fnu"):
                    for (
                        spec_type,
                        imgs,
                    ) in g.black_holes.images_psf_fnu.items():
                        for f, img in imgs.items():
                            self.images_flux_psf["BlackHole"].setdefault(
                                spec_type, {}
                            ).setdefault(f, []).append(img.arr * img.units)

        # Convert the lists of images to unyt arrays
        for spec_type, imgs in self.images_flux["Galaxy"].items():
            for f, img in imgs.items():
                self.images_flux["Galaxy"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_flux["Stars"].items():
            for f, img in imgs.items():
                self.images_flux["Stars"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_flux["BlackHole"].items():
            for f, img in imgs.items():
                self.images_flux["BlackHole"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_flux_psf.items():
            for f, img in imgs.items():
                self.images_flux_psf[spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_flux_psf["Stars"].items():
            for f, img in imgs.items():
                self.images_flux_psf["Stars"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_flux_psf["BlackHole"].items():
            for f, img in imgs.items():
                self.images_flux_psf["BlackHole"][spec_type][f] = unyt_array(
                    img
                )

        # Done!
        self._got_images_flux = True
        self._took(start, "Getting flux images")

    def get_lnu_data_cubes(self):
        """Compute the spectral luminosity density data cubes."""
        start = time.perf_counter()
        raise exceptions.NotImplemented(
            "Data cubes are not yet implemented in Surveys."
        )

        # Done!
        self._got_lnu_data_cubes = True
        self._took(start, "Getting lnu data cubes")

    def get_fnu_data_cubes(self):
        """Compute the Spectral flux density data cubes."""
        start = time.perf_counter()
        raise exceptions.NotImplemented(
            "Data cubes are not yet implemented in Surveys."
        )

        # Done!
        self._got_fnu_data_cubes = True
        self._took(start, "Getting fnu data cubes")

    def _run_extra_analysis(self):
        """
        Call any user provided analysis functions.

        We will call this just before writing out all data. This ensures that
        all data generated by the pipeline exists before performing the user
        calculations. This is important since the user may want to use the
        data generated by the pipeline in their analysis functions.
        """
        start = time.perf_counter()

        # Nothing to do if we have no analysis functions
        if len(self._analysis_funcs) == 0:
            return

        # Loop over the analysis functions and run them on each individual
        # galaxy. We can do this with a threadpool if we have multiple threads.
        for func, args, kwargs, key in zip(
            self._analysis_funcs,
            self._analysis_args,
            self._analysis_kwargs,
            self._analysis_results_keys,
        ):
            func_start = time.perf_counter()
            res = []
            for g in self.galaxies:
                res.append(func(g, *args, **kwargs))
            self._analysis_results[key] = res
            self._took(func_start, f"{key} extra analysis")

        # Done!
        self._took(start, "Extra analysis")

    def _parallel_write(self, outpath):
        """
        Collect and write the data in parallel.

        This function is used when running with MPI. If we have parallel h5py
        we will redirect to _treu_parallel_write, otherwise we will bring all
        the data to rank 0 and write it out there.

        If we don't have true parallel writing we will gather the data and then
        sort it before writing it out.

        Args:
            outpath (str):
                The path to the HDF5 file to write.
        """
        # If we have parallel h5py we can write in parallel using MPI,
        # redirect to the function that does this. TODO
        if hasattr(h5py.get_config(), "mpi") and h5py.get_config().mpi:
            warn(
                "True parallel writing across multiple ranks is coming soon."
                "Falling back to rank 0 writing."
            )
            # self._true_parallel_write(
            #     outpath,
            #     rank_output,
            #     out_paths,
            #     attr_paths,
            # )
            # return

        # Before we do anything we need to collect together the galaxy indices
        # for all ranks, we can do this here separately because the order of
        # a gather is guaranteed.
        sinds = self.comm.gather(self.galaxy_indices, root=0)
        sinds = np.concatenate(sinds) if self.rank == 0 else None

        # We'll collect and write each dataset we have actually computed but
        # while doing so we need to ensure everything is correctly sorted.
        # We'll chain these operations together in every call below where
        # everything is done recursively to avoid writing out the same code
        # multiple times.

        # Write spectral luminosity densities (we'll collect these
        # separately since they are the most memory intensive)
        if self._got_lnu_spectra:
            gal_data = sort_data_recursive(
                recursive_gather(self.lnu_spectra["Galaxy"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        gal_data,
                        "Galaxies/Spectra/SpectralLuminosityDensity",
                    )
            del gal_data
            star_data = sort_data_recursive(
                recursive_gather(self.lnu_spectra["Stars"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        star_data,
                        "Galaxies/Stars/Spectra/SpectralLuminosityDensity",
                    )
            del star_data
            bh_data = sort_data_recursive(
                recursive_gather(self.lnu_spectra["BlackHole"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        bh_data,
                        "Galaxies/BlackHoles/Spectra/SpectralLuminosityDensity",
                    )
            del bh_data

        # Write spectral flux densities (we'll collect these separately
        # since they are the most memory intensive)
        if self._got_fnu_spectra:
            gal_data = sort_data_recursive(
                recursive_gather(self.fnu_spectra["Galaxy"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        gal_data,
                        "Galaxies/Spectra/SpectralFluxDensity",
                    )
            del gal_data
            star_data = sort_data_recursive(
                recursive_gather(self.fnu_spectra["Stars"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        star_data,
                        "Galaxies/Stars/Spectra/SpectralFluxDensity",
                    )
            del star_data
            bh_data = sort_data_recursive(
                recursive_gather(self.fnu_spectra["BlackHole"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        bh_data,
                        "Galaxies/BlackHoles/Spectra/SpectralFluxDensity",
                    )
            del bh_data

        # Write photometric luminosities
        if self._got_luminosities:
            data = sort_data_recursive(
                recursive_gather(self.luminosities["Galaxy"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        data["Galaxy"],
                        "Galaxies/Photometry/Luminosities",
                    )
                    write_datasets_recursive(
                        hdf,
                        data["Stars"],
                        "Galaxies/Stars/Photometry/Luminosities",
                    )
                    write_datasets_recursive(
                        hdf,
                        data["BlackHole"],
                        "Galaxies/BlackHoles/Photometry/Luminosities",
                    )

        # Write photometric fluxes
        if self._got_fluxes:
            data = sort_data_recursive(
                recursive_gather(self.fluxes["Galaxy"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        data["Galaxy"],
                        "Galaxies/Photometry/Fluxes",
                    )
                    write_datasets_recursive(
                        hdf,
                        data["Stars"],
                        "Galaxies/Stars/Photometry/Fluxes",
                    )
                    write_datasets_recursive(
                        hdf,
                        data["BlackHole"],
                        "Galaxies/BlackHoles/Photometry/Fluxes",
                    )

        # Write emission line luminosities
        if self._got_lum_lines:
            data = sort_data_recursive(
                recursive_gather(self.lines_lum["Galaxy"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        data["Galaxy"],
                        "Galaxies/Lines/Luminosity",
                    )
                    write_datasets_recursive(
                        hdf,
                        data["Stars"],
                        "Galaxies/Stars/Lines/Luminosity",
                    )
                    write_datasets_recursive(
                        hdf,
                        data["BlackHole"],
                        "Galaxies/BlackHoles/Lines/Luminosity",
                    )

        # Write emission line continuum luminosities
        if self._got_lum_lines:
            data = sort_data_recursive(
                recursive_gather(self.line_cont_lum["Galaxy"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        data["Galaxy"],
                        "Galaxies/Lines/Continuum",
                    )
                    write_datasets_recursive(
                        hdf,
                        data["Stars"],
                        "Galaxies/Stars/Lines/Continuum",
                    )
                    write_datasets_recursive(
                        hdf,
                        data["BlackHole"],
                        "Galaxies/BlackHoles/Lines/Continuum",
                    )

        # Write luminosity images (again these are heavy so we'll collect
        # them separately)
        if self._got_images_lum:
            gal_data = sort_data_recursive(
                recursive_gather(self.images_lum["Galaxy"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        gal_data,
                        "Galaxies/Images/Luminosity",
                    )
            del gal_data
            star_data = sort_data_recursive(
                recursive_gather(self.images_lum["Stars"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        star_data,
                        "Galaxies/Stars/Images/Luminosity",
                    )
            del star_data
            bh_data = sort_data_recursive(
                recursive_gather(self.images_lum["BlackHole"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        bh_data,
                        "Galaxies/BlackHoles/Images/Luminosity",
                    )
            del bh_data
            gal_data = sort_data_recursive(
                recursive_gather(self.images_lum_psf["Galaxy"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        gal_data,
                        "Galaxies/PSFImages/Luminosity",
                    )
            del gal_data
            star_data = sort_data_recursive(
                recursive_gather(self.images_lum_psf["Stars"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        star_data,
                        "Galaxies/Stars/PSFImages/Luminosity",
                    )
            del star_data
            bh_data = sort_data_recursive(
                recursive_gather(self.images_lum_psf["BlackHole"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        bh_data,
                        "Galaxies/BlackHoles/PSFImages/Luminosity",
                    )
            del bh_data

        # Write flux images (again these are heavy so we'll collect them
        # separately)
        if self._got_images_flux:
            gal_data = sort_data_recursive(
                recursive_gather(self.images_flux["Galaxy"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        gal_data,
                        "Galaxies/Images/Flux",
                    )
            del gal_data
            star_data = sort_data_recursive(
                recursive_gather(self.images_flux["Stars"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        star_data,
                        "Galaxies/Stars/Images/Flux",
                    )
            del star_data
            bh_data = sort_data_recursive(
                recursive_gather(self.images_flux["BlackHole"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        bh_data,
                        "Galaxies/BlackHoles/Images/Flux",
                    )
            del bh_data
            gal_data = sort_data_recursive(
                recursive_gather(self.images_flux_psf["Galaxy"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        gal_data,
                        "Galaxies/PSFImages/Flux",
                    )
            del gal_data
            star_data = sort_data_recursive(
                recursive_gather(self.images_flux_psf["Stars"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        star_data,
                        "Galaxies/Stars/PSFImages/Flux",
                    )
            del star_data
            bh_data = sort_data_recursive(
                recursive_gather(self.images_flux_psf["BlackHole"], self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        bh_data,
                        "Galaxies/BlackHoles/PSFImages/Flux",
                    )
            del bh_data

        # Write out the extra analysis results
        if len(self._analysis_results) > 0:
            data = sort_data_recursive(
                recursive_gather(self._analysis_results, self.comm),
                sinds,
            )
            if self.rank == 0:
                with h5py.File(outpath, "a") as hdf:
                    write_datasets_recursive(
                        hdf,
                        data,
                        "Galaxies",
                    )
            del data

    def _true_parallel_write(self, outpath):
        """
        Write the data in parallel using MPI.

        This function is used when running with MPI and parallel h5py.

        This function is only called if we know we have parallel h5py, all
        ranks will collect together their data and then write to the HDF5 file
        in parallel.

        Args:
            outpath (str):
                The path to the HDF5 file to write.
        """
        raise NotImplementedError(
            "True parallel writing is not yet implemented."
        )

    def _serial_write(self, outpath):
        """
        Write the data in serial.

        This is the function used when not running with MPI.

        We'll sort the data by galaxy index before we write it. This ensures
        we have a consistent ordering of the data based on the user specified
        indices.

        Args:
            outpath (str):
                The path to the HDF5 file to write.
        """
        # We'll write each dataset we have actually computed but while doing
        # so we need to ensure everything is correctly sorted. We'll chain
        # these operations together in every call below where everything is
        # done recursively to avoid writing out the same code multiple times.
        with h5py.File(outpath, "a") as hdf:
            # Write spectral luminosity densities
            if self._got_lnu_spectra:
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.lnu_spectra["Galaxy"], self.galaxy_indices
                    ),
                    "Galaxies/Spectra/SpectralLuminosityDensity",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.lnu_spectra["Stars"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Stars/Spectra/SpectralLuminosityDensity",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.lnu_spectra["BlackHole"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/BlackHoles/Spectra/SpectralLuminosityDensity",
                )

            # Write spectral flux densities
            if self._got_fnu_spectra:
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.fnu_spectra["Galaxy"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Spectra/SpectralFluxDensity",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.fnu_spectra["Stars"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Stars/Spectra/SpectralFluxDensity",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.fnu_spectra["BlackHole"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/BlackHoles/Spectra/SpectralFluxDensity",
                )

            # Write photometric luminosities
            if self._got_luminosities:
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.luminosities["Galaxy"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Photometry/Luminosities",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.luminosities["Stars"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Stars/Photometry/Luminosities",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.luminosities["BlackHole"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/BlackHoles/Photometry/Luminosities",
                )

            # Write photometric fluxes
            if self._got_fluxes:
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.fluxes["Galaxy"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Photometry/Fluxes",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.fluxes["Stars"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Stars/Photometry/Fluxes",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.fluxes["BlackHole"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/BlackHoles/Photometry/Fluxes",
                )

            # Write emission line luminosities
            if self._got_lum_lines:
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.lines_lum["Galaxy"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Lines/Luminosity",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.lines_lum["Stars"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Stars/Lines/Luminosity",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.lines_lum["BlackHole"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/BlackHoles/Lines/Luminosity",
                )

            # Write emission line continuum luminosities
            if self._got_lum_lines:
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.line_cont_lum["Galaxy"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Lines/Continuum",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.line_cont_lum["Stars"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Stars/Lines/Continuum",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.line_cont_lum["BlackHole"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/BlackHoles/Lines/Continuum",
                )

            # Write luminosity images
            if self._got_images_lum:
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.images_lum["Galaxy"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Images/Luminosity",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.images_lum["Stars"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Stars/Images/Luminosity",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.images_lum["BlackHole"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/BlackHoles/Images/Luminosity",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.images_lum_psf["Galaxy"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/PSFImages/Luminosity",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.images_lum_psf["Stars"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Stars/PSFImages/Luminosity",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.images_lum_psf["BlackHole"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/BlackHoles/PSFImages/Luminosity",
                )

            # Write flux images
            if self._got_images_flux:
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.images_flux["Galaxy"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Images/Flux",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.images_flux["Stars"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Stars/Images/Flux",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.images_flux["BlackHole"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/BlackHoles/Images/Flux",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.images_flux_psf["Galaxy"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/PSFImages/Flux",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.images_flux_psf["Stars"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/Stars/PSFImages/Flux",
                )
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self.images_flux_psf["BlackHole"],
                        self.galaxy_indices,
                    ),
                    "Galaxies/BlackHoles/PSFImages/Flux",
                )

            # Write out the extra analysis results
            if len(self._analysis_results) > 0:
                write_datasets_recursive(
                    hdf,
                    sort_data_recursive(
                        self._analysis_results,
                        self.galaxy_indices,
                    ),
                    "Galaxies",
                )

    def write(self, outpath, particle_datasets=False):
        """
        Write what we have produced to a HDF5 file.

        First we collect everything up into a dictionary where all the galaxy
        data is stored in contiguous arrays. In MPI land when then gather all
        these individual arrays together. Finally, we write the dictionary
        out to a HDF5 file.

        Args:
            outpath (str):
                The path to the HDF5 file to write.
            particle_datasets (bool, optional):
                Whether to write out particle datasets. Default is False.
                [Currently unsupported]
        """
        # Particle datasets are not yet implemented
        if particle_datasets:
            raise exceptions.NotImplemented(
                "Particle datasets are not yet implemented."
            )

        # We're done with everything so we know we'll have what is needed for
        # any extra analysis asked for by the user. We'll run these now.
        self._run_extra_analysis()

        # In MPI land just wait until everyone has made it here
        if self.using_mpi:
            self._print(f"Rank {self.rank} waiting to write.")
            self.comm.barrier()

        # Regardless of parallel HDF5, we first need to create our file, some
        # basic structure and store some top level metadata (we will
        # overwrite any existing file with the same name)
        if self.rank == 0:
            with h5py.File(outpath, "w") as hdf:
                # Write out some top level metadata
                hdf.attrs["synthesizer_version"] = __version__

                # Create groups for the instruments, emission model, and
                # galaxies
                inst_group = hdf.create_group("Instruments")
                model_group = hdf.create_group("EmissionModel")
                hdf.create_group("Galaxies")  # we'll use this in a mo

                # Write out the instruments
                inst_group.attrs["ninstruments"] = (
                    self.instruments.ninstruments
                )
                for label, instrument in self.instruments.items():
                    instrument.to_hdf5(inst_group.create_group(label))

                # Write out the emission model
                for label, model in self.emission_model.items():
                    model.to_hdf5(model_group.create_group(label))

        # Call the appropriate galaxy property writer based on whether we
        # have parallel h5py
        write_start = time.perf_counter()
        if self.using_mpi:
            self._parallel_write(outpath)
        else:
            self._serial_write(outpath)
        self._took(write_start, f"Writing output to {outpath}")

        # Totally done!
        self._say_goodbye()

    def repartition_galaxies(self, galaxy_weights=None, random_seed=42):
        """Given the galaxies repartition them across the ranks."""
        raise NotImplementedError("Repartitioning is not yet implemented.")

    def _report_balance(self):
        """
        Report the balance of galaxies across the ranks.

        This function will print out a nice horizontal bar graph showing the
        distribution of galaxies across the ranks.
        """
        # Communicate local counts to rank 0
        counts = self.comm.gather(self.n_galaxies_local)
        print(counts)

        # Produce a nice horizontal bar graph to show the
        # distribution of galaxies across the ranks. This only needs printing
        # on rank 0 regardless of verbosity.
        if self.rank == 0:
            self._print("Partitioned galaxies across ranks:")
            # Find the maximum list length for scaling
            max_count = max(counts)

            for rank, count in enumerate(counts):
                # Calculate the length of the bar based on the relative size
                bar_length = int((count / max_count) * 50)

                # Create the bar and append the list length in brackets
                bar = "#" * bar_length
                self._print(
                    f"Rank {str(rank).zfill(len(str(self.size)) + 1)} - "
                    f"{bar} ({count})"
                )
