{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947397f6",
   "metadata": {},
   "source": [
    "# Images From Galaxy Particle distributions\n",
    "\n",
    "Synthesizer can create various different types of images and maps from particle data.\n",
    "\n",
    "In the example below we demonstrate this for a galaxy stellar distribution taken from a CAMELS simulation galaxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dcb7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.colors as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.cosmology import Planck18 as cosmo\n",
    "from scipy import signal\n",
    "from synthesizer.emission_models import IncidentEmission\n",
    "from synthesizer.filters import FilterCollection as Filters\n",
    "from synthesizer.grid import Grid\n",
    "from synthesizer.kernel_functions import Kernel\n",
    "from synthesizer.load_data.load_camels import load_CAMELS_IllustrisTNG\n",
    "from unyt import Hz, erg, kpc, s\n",
    "\n",
    "# Define the grid\n",
    "grid_name = \"test_grid\"\n",
    "grid_dir = \"../../../tests/test_grid/\"\n",
    "grid = Grid(grid_name, grid_dir=grid_dir, new_lam=np.logspace(2, 5, 600))\n",
    "\n",
    "# Create galaxy object\n",
    "gal = load_CAMELS_IllustrisTNG(\n",
    "    \"../../../tests/data/\",\n",
    "    snap_name=\"camels_snap.hdf5\",\n",
    "    group_name=\"camels_subhalo.hdf5\",\n",
    "    physical=True,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f970eff",
   "metadata": {},
   "source": [
    "### Getting the photometry\n",
    "\n",
    "To make an image we need to generate spectra for each individual star particle. To do this we use the galaxy's in built `generate_particle_spectra` method, and create `\"total\"` SEDs with both stellar and nebular contributions. \n",
    "\n",
    "We can then calculate the photometry on these spectra by defining a filter collection, and calculating the luminosities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c67b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the stellar rest frame SEDs for all particles in erg / s / Hz\n",
    "model = IncidentEmission(grid)\n",
    "sed = gal.stars.get_particle_spectra(model)\n",
    "\n",
    "# Calculate the observed SED in nJy\n",
    "sed.get_fnu(cosmo, gal.redshift, igm=None)\n",
    "\n",
    "filter_codes = [\n",
    "    \"JWST/NIRCam.F090W\",\n",
    "    \"JWST/NIRCam.F150W\",\n",
    "    \"JWST/NIRCam.F200W\",\n",
    "]\n",
    "\n",
    "filters = Filters(filter_codes, new_lam=grid.lam)\n",
    "\n",
    "gal.stars.particle_spectra[\"incident\"].get_photo_lnu(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fefc73",
   "metadata": {},
   "source": [
    "## Imaging\n",
    "\n",
    "The last step before we can make any images is to define the resolution of our images and the FOV (or width) of the images. These must have units associated to them to enable the code to internally transform all quantites to a consistent unit system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abb509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the width of the image\n",
    "width = 30 * kpc\n",
    "\n",
    "# Define image resolution (here we arbitrarily set it to 100\n",
    "# pixels along an axis)\n",
    "resolution = width / 200\n",
    "\n",
    "print(\n",
    "    \"Image width is %.2f kpc with %.2f kpc resolution\"\n",
    "    % (width.value, resolution.value)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e424d79",
   "metadata": {},
   "source": [
    "Now we have everything we need to make images. Although it is possible to work with the low level `ImageCollection` and `Image` methods, here we use the interface on a `Galaxy`. There are two of these helper methods, `get_imgs_luminosity` for luminosity images, and `get_imgs_flux` for flux images. We will focus on the former here.\n",
    "\n",
    "The image helper methods both take the image properties we previously defined, and a spectra type. These types can be any spectra for which you have calculated photometry, e.g. `\"incident\"`, `\"intrinsic\"`, or `\"attenuated\"`, for either stars (`stellar_photometry`) or black holes (`blackhole_photometry`). If both a stellar and black hole photometry type are passed then an image is made for each component before they are combined and returned.\n",
    "\n",
    "Images can either be simple 2D histograms, or the particles can be smoothed over their kernels. What type of image is made is controlled by the `img_type` argument. Below we demonstrate both approaches. However, for the latter we also need to define a kernel, which we have already imported from the `kernel_functions` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfa17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the SPH kernel\n",
    "sph_kernel = Kernel()\n",
    "kernel_data = sph_kernel.get_kernel()\n",
    "\n",
    "# Get the image\n",
    "hist_imgs = gal.get_images_luminosity(\n",
    "    resolution,\n",
    "    fov=width,\n",
    "    img_type=\"hist\",\n",
    "    stellar_photometry=\"incident\",\n",
    "    blackhole_photometry=None,\n",
    ")\n",
    "\n",
    "# Get the image\n",
    "smooth_imgs = gal.get_images_luminosity(\n",
    "    resolution,\n",
    "    fov=width,\n",
    "    img_type=\"smoothed\",\n",
    "    stellar_photometry=\"incident\",\n",
    "    blackhole_photometry=None,\n",
    "    kernel=kernel_data,\n",
    "    kernel_threshold=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2ce8c",
   "metadata": {},
   "source": [
    "When making images in multiple bands, the image arrays themselves are stored on the returned `ImageCollection` in a dictionary called `imgs`, of the form `{filter_code: Image}`.\n",
    "Here, an `Image` is a container including the image array itself (`arr`), unit information (`units`) and the `resolution` and `fov` of the image. An `Image` object also includes a number of different methods for manipulating and visualising individual images. \n",
    "\n",
    "Below we will extract this dictionary and plot each of the images we have made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets set up a simple normalisation across all images\n",
    "vmax = 0\n",
    "for img in hist_imgs.values():\n",
    "    up = np.percentile(img.arr, 99.9)\n",
    "    if up > vmax:\n",
    "        vmax = up\n",
    "hist_norm = cm.Normalize(vmin=0, vmax=vmax)\n",
    "vmax = 0\n",
    "for img in smooth_imgs.values():\n",
    "    up = np.percentile(img.arr, 99.9)\n",
    "    if up > vmax:\n",
    "        vmax = up\n",
    "smooth_norm = cm.Normalize(vmin=0, vmax=vmax)\n",
    "\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure(figsize=(4 * len(filters), 4 * 2))\n",
    "gs = gridspec.GridSpec(2, len(filters), hspace=0.0, wspace=0.0)\n",
    "\n",
    "# Create top row\n",
    "axes = []\n",
    "for i in range(len(filters)):\n",
    "    axes.append(fig.add_subplot(gs[0, i]))\n",
    "\n",
    "# Loop over images plotting them\n",
    "for ax, fcode in zip(axes, filter_codes):\n",
    "    ax.imshow(hist_imgs[fcode].arr, norm=hist_norm, cmap=\"Greys_r\")\n",
    "    ax.set_title(fcode)\n",
    "    ax.tick_params(\n",
    "        axis=\"both\",\n",
    "        which=\"both\",\n",
    "        left=False,\n",
    "        right=False,\n",
    "        labelleft=False,\n",
    "        labelright=False,\n",
    "        bottom=False,\n",
    "        top=False,\n",
    "        labelbottom=False,\n",
    "        labeltop=False,\n",
    "    )\n",
    "\n",
    "# Set y axis label on left most plot\n",
    "axes[0].set_ylabel(\"Histogram\")\n",
    "\n",
    "# Create bottom row\n",
    "axes = []\n",
    "for i in range(len(filters)):\n",
    "    axes.append(fig.add_subplot(gs[1, i]))\n",
    "\n",
    "# Loop over images plotting them\n",
    "for ax, fcode in zip(axes, filter_codes):\n",
    "    ax.imshow(smooth_imgs[fcode].arr, norm=smooth_norm, cmap=\"Greys_r\")\n",
    "    ax.tick_params(\n",
    "        axis=\"both\",\n",
    "        which=\"both\",\n",
    "        left=False,\n",
    "        right=False,\n",
    "        labelleft=False,\n",
    "        labelright=False,\n",
    "        bottom=False,\n",
    "        top=False,\n",
    "        labelbottom=False,\n",
    "        labeltop=False,\n",
    "    )\n",
    "\n",
    "# Set y axis label on left most plot\n",
    "axes[0].set_ylabel(\"Smoothed\")\n",
    "\n",
    "# Plot the image\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a548ce",
   "metadata": {},
   "source": [
    "## Applying a Point Spread Function (PSF)\n",
    "\n",
    "To properly model observations from a particular instrument we must take into account the point spread function (PSF). \n",
    "\n",
    "To apply a PSF we can either use `Image.apply_psf` on individual `Image` objects, or apply a dictionary of PSFs, of the form `{filter_code: psf_array}`, to each image in an `ImageCollection` with the `apply_psfs` method. Here we will just create a fake gaussian PSF for all filters, but PSFs can be sourced however the user wishes (for Webb we recommend the _webbpsf_ package), as long as a simple numpy array is passed within the psf dictionary for each filter.\n",
    "\n",
    "To get the most accurate result from the PSF convolution it is recommended to do the convolution on a super-sampled image (i.e. much higher resolution than the PSF). Although-here we just supersample the images we have already made, it is recommended to first make the images at the super-sampled resolution, and then subsequently downsample after the fact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake PSF for each filter\n",
    "psf = np.outer(\n",
    "    signal.windows.gaussian(100, 3), signal.windows.gaussian(100, 3)\n",
    ")\n",
    "psfs = {f: psf for f in filters.filter_codes}\n",
    "\n",
    "# Supersample the image\n",
    "smooth_imgs.supersample(2)\n",
    "\n",
    "# Apply the PSFs\n",
    "psf_imgs = smooth_imgs.apply_psfs(psfs)\n",
    "\n",
    "# And downsample back to the native resolution\n",
    "smooth_imgs.downsample(0.5)\n",
    "psf_imgs.downsample(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb121d1c",
   "metadata": {},
   "source": [
    "`apply_psfs` returns a new `ImageCollection` containing the newly convolved `Image` objects. We can now use the plotting helper function to plot these images, with some normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847dac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets set up a simple normalisation across all images\n",
    "vmax = 0\n",
    "for img in psf_imgs.values():\n",
    "    up = np.percentile(img.arr, 99.9)\n",
    "    if up > vmax:\n",
    "        vmax = up\n",
    "\n",
    "# Get the plot\n",
    "fig, ax = psf_imgs.plot_images(show=True, vmin=0, vmax=vmax)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c16ea",
   "metadata": {},
   "source": [
    "## Applying noise\n",
    "\n",
    "The final ingredient for a fully forward modelled synthetic image is a noise field. We include 4 different to approaches to include noise: \n",
    "- `apply_noise_array`: Add an existing noise field / array.\n",
    "- `apply_noise_from_std`: Derive a noise distribution, centered on 0, given a user specified standard deviation, and then generate and add a noise array. \n",
    "- `apply_noise_from_snr` (aperture): Derive a noise distribution from a Signal-to-Noise Ratio (SNR), defined in an aperture with size `aperture_radius` and a specified `depth`. This will derive the standard deviation of the noise distribution assuming $SNR= S / \\sigma$ for an aperture, before deriving the per pixel noise, computing the noise array and adding it.\n",
    "- `apply_noise_from_snr` (point source): Derive a noise distribution from a SNR and depth. This will derive the standard deviation of the noise distribution assuming $SNR= S / \\sigma$ for a pixel before computing the noise array and adding it. This behaviour can be achieved by omitting `aperture_radius` in the call to `apply_noise_from_snr`\n",
    "    \n",
    "As with applying a PSF, these methods have singular versions (as listed above) which can be used on an individual `Image`, and pluralised versions which can be used on an `ImageCollection`, and take dictionaries for each of the arguments. \n",
    "\n",
    "If an image has units then the passed `noise_arr` or `noise_std` must also have units.\n",
    "\n",
    "Applying noise with any of the methods described above will return a new `ImageCollection` / `Image` containing the noisy image. In addition to the noisy image (stored under `Image.arr`) the new `Image` (or the new `Image` objects within an `ImageCollection`) will contain the noise array stored in the `noise_arr` attribute, and the weight map stored in the `weight_map` attribute on the `Image`.\n",
    "\n",
    "Below we demonstrate each method using the `ImageCollection` interface. The noise and weight maps are stored in `Image.noise_arr` and `Image.weight_map`.\n",
    "\n",
    "### Noise arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396666e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_start = time.time()\n",
    "\n",
    "# Get a noise array for each filter\n",
    "noises = {\n",
    "    f: np.random.normal(loc=0, scale=10**26.0, size=(psf_imgs.npix))\n",
    "    * erg\n",
    "    / s\n",
    "    / Hz\n",
    "    for f in psf_imgs.keys()\n",
    "}\n",
    "\n",
    "# Apply the noise array\n",
    "noise_array_imgs = psf_imgs.apply_noise_arrays(noises)\n",
    "\n",
    "print(\"Noisy images made, took:\", time.time() - img_start)\n",
    "\n",
    "# Get the plot\n",
    "fig, ax = noise_array_imgs.plot_images(show=True, vmin=0, vmax=vmax)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d7b47",
   "metadata": {},
   "source": [
    "### Noise from standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_start = time.time()\n",
    "\n",
    "# Get a noise standard deviation for each filter\n",
    "noise_stds = {f: 10**26.3 * erg / s / Hz for f in psf_imgs.keys()}\n",
    "\n",
    "# Apply the noise array\n",
    "noise_std_imgs = psf_imgs.apply_noise_from_stds(noise_stds)\n",
    "\n",
    "print(\"Noisy images made, took:\", time.time() - img_start)\n",
    "\n",
    "# Get the plot\n",
    "fig, ax = noise_std_imgs.plot_images(show=True, vmin=0, vmax=vmax)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c132319",
   "metadata": {},
   "source": [
    "### Noise from an aperture depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionaries with the noise properties for each filter\n",
    "snrs = {f: 5 for f in psf_imgs.keys()}\n",
    "depths = {f: 10**28.0 * erg / s / Hz for f in psf_imgs.keys()}\n",
    "\n",
    "# Apply the noise array\n",
    "noise_app_imgs = psf_imgs.apply_noise_from_snrs(\n",
    "    snrs=snrs, depths=depths, aperture_radius=0.5 * kpc\n",
    ")\n",
    "\n",
    "print(\"Noisy images made, took:\", time.time() - img_start)\n",
    "\n",
    "# Get the plot\n",
    "fig, ax = noise_app_imgs.plot_images(show=True, vmin=0, vmax=vmax)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a9f27e",
   "metadata": {},
   "source": [
    "### Noise from point source depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionaries with the noise properties for each filter\n",
    "snrs = {f: 5 for f in psf_imgs.keys()}\n",
    "depths = {f: 10**27.0 * erg / s / Hz for f in psf_imgs.keys()}\n",
    "\n",
    "# Apply the noise array\n",
    "noise_ps_imgs = psf_imgs.apply_noise_from_snrs(snrs=snrs, depths=depths)\n",
    "\n",
    "print(\"Noisy images made, took:\", time.time() - img_start)\n",
    "\n",
    "# Get the plot\n",
    "fig, ax = noise_ps_imgs.plot_images(show=True, vmin=0, vmax=vmax)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598bc9a",
   "metadata": {},
   "source": [
    "## Making an RGB image\n",
    "\n",
    "Finally we can use the RGB image method on the `ImageCollection` to make quick RGB images by simply providing a dictionary detailing which filters we want in which bands, and optional weights for each filter (which we will ignore here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the rgb image from the psf image above example using our 3 filters\n",
    "rgb_img = noise_ps_imgs.make_rgb_image(\n",
    "    rgb_filters={\n",
    "        \"R\": [\n",
    "            \"JWST/NIRCam.F200W\",\n",
    "        ],\n",
    "        \"G\": [\n",
    "            \"JWST/NIRCam.F150W\",\n",
    "        ],\n",
    "        \"B\": [\n",
    "            \"JWST/NIRCam.F090W\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Set up minima and maxima\n",
    "vmin = -np.percentile(rgb_img, 32)\n",
    "vmax = np.percentile(rgb_img, 99.9)\n",
    "norm = cm.Normalize(vmin=vmin, vmax=vmax, clip=True)\n",
    "print(\"Scaling to:\", vmin, \"->\", vmax)\n",
    "\n",
    "# Normalise the image.\n",
    "rgb_img = norm(rgb_img)\n",
    "\n",
    "print(rgb_img.shape)\n",
    "\n",
    "# Plot the image\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(rgb_img, origin=\"lower\", interpolation=\"nearest\")\n",
    "ax.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
