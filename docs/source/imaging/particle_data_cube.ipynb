{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Data Cubes from Galaxy Particle distributions\n",
    "\n",
    "In this example we show how to create a spectral data cube from particle data.\n",
    "\n",
    "We first load some demo CAMELS data and a grid, as demonstrated elsewhere in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from astropy.cosmology import Planck18 as cosmo\n",
    "from synthesizer.emission_models import IntrinsicEmission\n",
    "from synthesizer.grid import Grid\n",
    "from synthesizer.imaging import SpectralCube\n",
    "from synthesizer.kernel_functions import Kernel\n",
    "from synthesizer.load_data.load_camels import load_CAMELS_IllustrisTNG\n",
    "from unyt import kpc\n",
    "\n",
    "# Define the grid\n",
    "grid_name = \"test_grid\"\n",
    "grid_dir = \"../../../tests/test_grid/\"\n",
    "grid = Grid(grid_name, grid_dir=grid_dir)\n",
    "\n",
    "# Create galaxy object\n",
    "gal = load_CAMELS_IllustrisTNG(\n",
    "    \"../../../tests/data/\",\n",
    "    snap_name=\"camels_snap.hdf5\",\n",
    "    group_name=\"camels_subhalo.hdf5\",\n",
    "    physical=True,\n",
    ")[0]\n",
    "\n",
    "# Calculate the stellar rest frame SEDs for all particles in erg / s / Hz\n",
    "model = IntrinsicEmission(grid, fesc=0.1)\n",
    "sed = gal.stars.get_particle_spectra(model)\n",
    "\n",
    "# Calculate the observed SED in nJy\n",
    "sed.get_fnu(cosmo, gal.redshift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Data Cube Creation\n",
    "\n",
    "We now have most of the ingredients we need to generate a spectral data cube from our galaxy. The only parts that are missing are the wavelength array of our spectral data cube, its resolution and FOV, and a kernel for smoothing particles over. We'll define these below and move on to making the spectral data cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the width of the image\n",
    "width = 30 * kpc\n",
    "\n",
    "# Define image resolution (here we arbitrarily set it to 100\n",
    "# pixels along an axis)\n",
    "resolution = width / 200\n",
    "\n",
    "# Define the wavelength array\n",
    "lam = np.linspace(10**3.5, 10**4.5, 1000)\n",
    "\n",
    "print(\n",
    "    \"Data cube spatial width is %.2f kpc with a %.2f kpc spaxel resolution\"\n",
    "    % (width.value, resolution.value)\n",
    ")\n",
    "\n",
    "# Get the SPH kernel\n",
    "kernel = Kernel()\n",
    "kernel_data = kernel.get_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesizer allows you to make either a \"2D histogram\" data cube, where particles are sorted into individual pixels, or data cubes where particles are smoothed over their SPH kernels. We'll focus on the latter, but to do the former you simply call `get_data_cube_hist` with an `sed`, `coordinates` and the `Sed` quantity you want to populate the data cube with.\n",
    "\n",
    "To make a smoothed data cube we first instantiate the `SpectralCube`, and then call `get_data_cube_smoothed`, which takes a `sed`, `coordinates`, `smoothing_lengths`, a `kernel`, the threshold of the `kernel`, and the `Sed` quantity you want to populate the data cube with. The possible quantities are `\"lnu\"`, `\"luminosity\"` or `\"llam\"` for rest frame luminosities, or `\"fnu\"`, `\"flam\"` or `\"flux\"` for fluxes (the latter 3 require `get_fnu` or `get_fnu0` to have been called). We will make a cube populated with `\"fnu\"`, i.e. the spectral flux density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_start = time.time()\n",
    "\n",
    "# Get the data cube\n",
    "cube = SpectralCube(resolution=resolution, lam=lam, fov=width)\n",
    "\n",
    "# And get the cube itself\n",
    "cube.get_data_cube_smoothed(\n",
    "    sed,\n",
    "    coordinates=gal.stars.coordinates,\n",
    "    smoothing_lengths=gal.stars.smoothing_lengths,\n",
    "    kernel=kernel_data,\n",
    "    kernel_threshold=1,\n",
    "    quantity=\"fnu\",\n",
    ")\n",
    "\n",
    "print(\"Spectral data cube created, took:\", time.time() - cube_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. We now have a spectral data cube to analyse. We can visualise the data cube by making an animation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animate the data cube\n",
    "ani = cube.animate_data_cube(fps=240, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Galaxy helper method\n",
    "\n",
    "If you don't want to use the low level `SpectralCube` object we also include a helper method on a galaxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = gal.get_data_cube(\n",
    "    resolution,\n",
    "    width,\n",
    "    lam,\n",
    "    cube_type=\"hist\",\n",
    "    stellar_spectra=\"intrinsic\",\n",
    "    kernel=kernel,\n",
    "    kernel_threshold=1,\n",
    "    quantity=\"flux\",\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
